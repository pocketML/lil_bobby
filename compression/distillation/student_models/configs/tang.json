{
    "embedding-dim" : 25,
    "vocab-size": 5000,
    "dropout": 0.1,
    "lr": 1,
    "weight-decay": 0,
    "batch-size": 50,
    "cls-hidden-dim": 200,
    "encoder-hidden-dim": 150,
    "batch-first": true,
    "num-layers": 1,
    "bidirectional": true,
    "type": "lstm",
    "rho": 0.95,
    "embedding-type": "hash",
    "clip-grad": 30000,
    "embedding-freeze": false,
    "cls-init-range": 0.1,
    "emb-init-range": 0.1
}